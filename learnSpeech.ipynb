{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "\n",
    "from hmmlearn import hmm\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "import scipy.io.wavfile as wav\n",
    "import matplotlib.pyplot as plot\n",
    "import numpy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the tagged data directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero']\n"
     ]
    }
   ],
   "source": [
    "ghmms = {}\n",
    "data = {}\n",
    "datadirs = [f for f in listdir(\"data/\") if ('.' not in f)]\n",
    "print(datadirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data to get MFCCs for each data file. Then split data into test and training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed 1713 1199 514\n",
      "bird 1731 1211 520\n",
      "cat 1733 1213 520\n",
      "dog 1746 1222 524\n",
      "down 2359 1651 708\n",
      "eight 2352 1646 706\n",
      "five 2357 1649 708\n",
      "four 2372 1660 712\n",
      "go 2372 1660 712\n",
      "happy 1742 1219 523\n",
      "house 1750 1225 525\n",
      "left 2353 1647 706\n",
      "marvin 1746 1222 524\n",
      "nine 2364 1654 710\n",
      "no 2375 1662 713\n",
      "off 2357 1649 708\n",
      "on 2367 1656 711\n",
      "one 2370 1659 711\n",
      "right 2367 1656 711\n",
      "seven 2377 1663 714\n",
      "sheila 1734 1213 521\n",
      "six 2369 1658 711\n",
      "stop 2380 1666 714\n",
      "three 2356 1649 707\n",
      "tree 1733 1213 520\n",
      "two 2373 1661 712\n",
      "up 2375 1662 713\n",
      "wow 1745 1221 524\n",
      "yes 2377 1663 714\n",
      "zero 2376 1663 713\n"
     ]
    }
   ],
   "source": [
    "for directory in datadirs:\n",
    "    curdir = \"data/\" + directory\n",
    "    datafiles = [f for f in listdir(curdir) if isfile(join(curdir,f))]\n",
    "\n",
    "    dataForWord = []\n",
    "    for f in datafiles:\n",
    "        (rate, signal) = wav.read(curdir + \"/\" + f)\n",
    "        dataForWord.append(mfcc(signal, rate, winfunc=numpy.hamming))\n",
    "    \n",
    "    train, test = train_test_split(dataForWord, test_size=0.3)\n",
    "    data[directory] = {'train': train, 'test': test}\n",
    "    print(directory, len(datafiles), len(data[directory]['train']), len(data[directory]['test']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshape data in a way that is amenable to HMMLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed (116667, 13) 116667\n",
      "bird (117592, 13) 117592\n",
      "cat (117684, 13) 117684\n",
      "dog (118709, 13) 118709\n",
      "down (161040, 13) 161040\n",
      "eight (160387, 13) 160387\n",
      "five (161200, 13) 161200\n",
      "four (161947, 13) 161947\n",
      "go (161108, 13) 161108\n",
      "happy (118664, 13) 118664\n",
      "house (119221, 13) 119221\n",
      "left (161345, 13) 161345\n",
      "marvin (119366, 13) 119366\n",
      "nine (161659, 13) 161659\n",
      "no (161507, 13) 161507\n",
      "off (160965, 13) 160965\n",
      "on (160941, 13) 160941\n",
      "one (161192, 13) 161192\n",
      "right (161818, 13) 161818\n",
      "seven (162256, 13) 162256\n",
      "sheila (118683, 13) 118683\n",
      "six (162398, 13) 162398\n",
      "stop (162542, 13) 162542\n",
      "three (161087, 13) 161087\n",
      "tree (117822, 13) 117822\n",
      "two (161758, 13) 161758\n",
      "up (161236, 13) 161236\n",
      "wow (118358, 13) 118358\n",
      "yes (162218, 13) 162218\n",
      "zero (163044, 13) 163044\n"
     ]
    }
   ],
   "source": [
    "flatData = {}\n",
    "lengths = {}\n",
    "for directory in datadirs:\n",
    "    flatDataForWord = []\n",
    "    runLengthsForWord = []\n",
    "    for fileData in data[directory]['train']:\n",
    "        runLengthsForWord.append(len(fileData))\n",
    "        flatDataForWord += fileData.flatten().tolist()\n",
    "            \n",
    "    flatData[directory] = numpy.array(flatDataForWord).reshape(-1, 13)\n",
    "    lengths[directory] = runLengthsForWord\n",
    "    print(directory, flatData[directory].shape, sum(lengths[directory]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a word model with each training set that corresponds to a single word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116667, 13)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0489d50b07e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mghmm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGMMHMM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_mix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mghmm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatData\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mghmms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mghmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speech-recognition-8CgqZx8j/lib/python3.6/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, lengths)\u001b[0m\n\u001b[1;32m    434\u001b[0m                 self._accumulate_sufficient_statistics(\n\u001b[1;32m    435\u001b[0m                     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframelogprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposteriors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfwdlattice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m                     bwdlattice)\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# XXX must be before convergence check, because otherwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speech-recognition-8CgqZx8j/lib/python3.6/site-packages/hmmlearn/hmm.py\u001b[0m in \u001b[0;36m_accumulate_sufficient_statistics\u001b[0;34m(self, stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    588\u001b[0m                                           posteriors, fwdlattice, bwdlattice):\n\u001b[1;32m    589\u001b[0m         super(GMMHMM, self)._accumulate_sufficient_statistics(\n\u001b[0;32m--> 590\u001b[0;31m             stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgmms_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speech-recognition-8CgqZx8j/lib/python3.6/site-packages/hmmlearn/base.py\u001b[0m in \u001b[0;36m_accumulate_sufficient_statistics\u001b[0;34m(self, stats, X, framelogprob, posteriors, fwdlattice, bwdlattice)\u001b[0m\n\u001b[1;32m    624\u001b[0m                                  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransmat_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m                                  bwdlattice, framelogprob, lneta)\n\u001b[0;32m--> 626\u001b[0;31m             \u001b[0mstats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'trans'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlneta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speech-recognition-8CgqZx8j/lib/python3.6/site-packages/numpy/lib/utils.py\u001b[0m in \u001b[0;36mnewfunc\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;34m\"\"\"`arrayrange` is deprecated, use `arange` instead!\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdepdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mnewfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_set_function_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/speech-recognition-8CgqZx8j/lib/python3.6/site-packages/scipy/special/_logsumexp.py\u001b[0m in \u001b[0;36mlogsumexp\u001b[0;34m(a, axis, b, keepdims, return_sign)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"logsumexp\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \"\"\"Compute the log of the sum of exponentials of input elements.\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for directory in datadirs:\n",
    "    print(flatData[directory].shape)\n",
    "    ghmm = hmm.GMMHMM(n_mix=2, n_components=2, n_iter=10)\n",
    "    ghmm.fit(flatData[directory], lengths=lengths[directory])\n",
    "    ghmms[directory] = ghmm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output trained gmmhmms as a pickle file for later prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pickle.dump(ghmms, open(\"hmmset.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(fileData):\n",
    "    logOddsToKey = {}\n",
    "    for key in ghmms:\n",
    "        ghmm = ghmms[key]\n",
    "        logOdds = ghmms[key].score_samples(fileData)[0]\n",
    "        logOddsToKey[logOdds] = key\n",
    "    return logOddsToKey[max(logOddsToKey.keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeledTestData = []\n",
    "for key in data:\n",
    "    for fileData in data[key]['test']:\n",
    "        labeledTestData.append((fileData, key))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict word for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "successful = 0\n",
    "for test in labeledTestData:\n",
    "    prediction = predict(test[0])\n",
    "    if prediction == test[1]:\n",
    "        successful += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# data points: 19429\n",
      "success rate: 0.31761799372072674\n"
     ]
    }
   ],
   "source": [
    "print('# data points:', len(labeledTestData))\n",
    "print('success rate:', float(successful) / len(labeledTestData) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
